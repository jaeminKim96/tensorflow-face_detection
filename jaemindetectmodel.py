# -*- coding: utf-8 -*-
"""jaemindetectmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gbRzRf6HvEcPc9o5P59_OLnRTU_jbiqa
"""

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
import os
import numpy as np
from PIL import Image
import cv2

!pip install opencv-python-headless

def read_n_preprocess(image):
    image = cv2.resize(image, (224, 224))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = image.astype(np.float32) / 255.0
    return image


def create_model():
    # model = tf.keras.Sequential(
    #     [
    #         tf.keras.applications.MobileNetV2(
    #             input_shape=(224, 224, 3), include_top=False
    #         ),
    #         tf.keras.layers.GlobalAveragePooling2D(),
    #         tf.keras.layers.Dense(1, activation="sigmoid"),
    #     ]
    # )
    # model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    # return model
    base_model = tf.keras.applications.MobileNetV2(input_shape=(224,224,3),include_top=False)

    for layer in base_model.layers:
        layer.trainable = False

    model = tf.keras.Sequential([base_model,
                                 tf.keras.layers.GlobalAveragePooling2D(),
                                 tf.keras.layers.Dense(256,activation='relu'),
                                 tf.keras.layers.BatchNormalization(),
                                 tf.keras.layers.Dropout(0.5),
                                 tf.keras.layers.Dense(1,activation='sigmoid')
                                 ])    

    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    
    return model

train_data_dir = "/content/gdrive/MyDrive/Colab Notebooks/train_data1"
val_data_dir = "/content/gdrive/MyDrive/Colab Notebooks/val_data1"

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=read_n_preprocess,
    horizontal_flip=True,
    rotation_range=20,
    zoom_range=0.1,
    width_shift_range=0.1,
    height_shift_range=0.1,
)

val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    preprocessing_function=read_n_preprocess
)

train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(224, 224),
    batch_size=16,
    class_mode="binary",
    classes=["nonperson", "person"],
    shuffle=True,
)
val_generator = val_datagen.flow_from_directory(
    val_data_dir,
    target_size=(224, 224),
    batch_size=16,
    class_mode="binary",
    classes=["nonperson", "person"],
    shuffle=True,
)

model = create_model()

final = model.fit(train_generator, epochs=50, validation_data=val_generator)

model.save("/content/gdrive/MyDrive/Colab Notebooks/face_detection_final.h5")

import random
import tqdm
import cv2
import glob

model = tf.keras.models.load_model("/content/gdrive/MyDrive/Colab Notebooks/face_detection_final.h5")

def represent_data_gen():
    # image_paths = glob.glob("/content/gdrive/MyDrive/Colab Notebooks/train_data1/person/*.jpg")
    # random.shuffle(image_paths)
    person_image_paths = glob.glob("/content/gdrive/MyDrive/Colab Notebooks/train_data1/person/*.jpg")
    nonperson_image_paths = glob.glob("/content/gdrive/MyDrive/Colab Notebooks/train_data1/nonperson/*.jpg")
    image_paths = person_image_paths + nonperson_image_paths

    for image in tqdm.tqdm(image_paths):
        img = cv2.imread(image)
        img = cv2.resize(img,dsize=(224,224),interpolation=cv2.INTER_AREA)
        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
        #img = np.expand_dims(img,-1)
        #img = np.expand_dims(img,0)
        img = img.astype(np.float32)/255.0
        img = np.expand_dims(img,axis=0)

        yield [img]

input_shape = (1,224,224,3)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = represent_data_gen
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.target_spec.supported_types = [tf.int8]
converter.allow_custom_ops = True

tflite_model = converter.convert()

open("/content/gdrive/MyDrive/Colab Notebooks/detect_both_quantize.tflite","wb").write(tflite_model)